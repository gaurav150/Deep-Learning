{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":91715,"databundleVersionId":11351736,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Dataset Description\nThe dataset for this competition (both train and test) was generated from a deep learning model trained on the Podcast Listening Time Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n\nFiles\ntrain.csv - the training dataset; Listening_Time_minutes is the target <br>\ntest.csv - the test dataset; your objective is to predict the Listening_Time_minutes for each row <br>\nsample_submission.csv - a sample submission file in the correct format.","metadata":{}},{"cell_type":"code","source":"!pip install lightgbm\n!pip install xgboost\n!pip install plotly","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:11:27.177103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport pickle as pk\nimport warnings\nwarnings.simplefilter(action='ignore',category=Warning)\n%matplotlib inline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv(r'/kaggle/input/playground-series-s5e4/train.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.read_csv(r'/kaggle/input/playground-series-s5e4/sample_submission.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.drop(columns='id',axis=1,inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test Dataset Manipulation","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(r'/kaggle/input/playground-series-s5e4/test.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = test_df.drop(columns='id',axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"test_df.head()","metadata":{}},{"cell_type":"code","source":"test.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training Data Shape is \",train.shape)\nprint(\"Testing Data Shape is \",test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Function for EDA visualizations\ndef plot_eda(df, target_col=None):\n    \"\"\"Generate exploratory data analysis plots\"\"\"\n    # Missing values heatmap\n    plt.figure(figsize=(12, 6))\n    sns.heatmap(df.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n    plt.title('Missing Values Heatmap')\n    plt.tight_layout()\n    plt.show()\n    \n    # Missing values percentages\n    missing_percent = df.isnull().mean().sort_values(ascending=False) * 100\n    plt.figure(figsize=(12, 6))\n    missing_percent[missing_percent > 0].plot(kind='bar')\n    plt.title('Percentage of Missing Values by Feature')\n    plt.ylabel('Percentage')\n    plt.tight_layout()\n    plt.show()\n    \n    if target_col and target_col in df.columns:\n        # Target distribution (for regression, use histogram)\n        plt.figure(figsize=(10, 5))\n        sns.histplot(df[target_col], bins=30, kde=True)\n        plt.title(f'Distribution of {target_col}')\n        plt.xlabel(target_col)\n        plt.ylabel('Frequency')\n        plt.tight_layout()\n        plt.show()\n        \n        # Correlation heatmap for numerical features\n        numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n        plt.figure(figsize=(12, 10))\n        correlation = df[numerical_cols].corr()\n        mask = np.triu(correlation)\n        sns.heatmap(correlation, annot=True, fmt=\".2f\", cmap=\"coolwarm\", mask=mask)\n        plt.title('Correlation Heatmap of Numerical Features')\n        plt.tight_layout()\n        plt.show()\n        \n        # Scatter plots for numerical features against target\n        for col in numerical_cols:\n            if col != target_col:\n                plt.figure(figsize=(12, 6))\n                sns.scatterplot(x=df[col], y=df[target_col])\n                plt.title(f'{col} vs {target_col}')\n                plt.xlabel(col)\n                plt.ylabel(target_col)\n                plt.tight_layout()\n                plt.show()\n\n        # Feature relationships with target for categorical features\n        categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n        categorical_cols = [col for col in categorical_cols if col != target_col and df[col].nunique() < 10]\n        \n        for col in categorical_cols[:3]:  # Limit to first 3 categorical features\n            plt.figure(figsize=(12, 6))\n            sns.boxplot(x=col, y=target_col, data=df)\n            plt.title(f'{col} vs {target_col}')\n            plt.xticks(rotation=45)\n            plt.tight_layout()\n            plt.show()\n\n# Run EDA on train data\nplot_eda(train, 'Listening_Time_minutes')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_eda(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(train.isnull().mean())*100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(test.isnull().mean())*100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data PreProcessing","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## For Train Data","metadata":{}},{"cell_type":"code","source":"## getting all different type of feature\nnum_features = [feature for feature in train.columns if  train[feature].dtype != 'O']\nprint(\"Number of numerical features : \",len(num_features))\ncat_features = [feature for feature in train.columns if train[feature].dtype == 'O']\nprint(\"Number of categorical features : \",len(cat_features))\ndiscrete_features = [feature for feature in num_features if len(train[feature].unique()) <= 25]\nprint(\"Number of discrete features : \",len(discrete_features))\ncontinuous_features  = [feature for feature in num_features if len(train[feature].unique()) > 25]\nprint(\"Number of continuous features :  \",len(continuous_features))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## check Missing Values \n### these are the features with nan value\nfeatures_with_nan = [features for features in train.columns if train[features].isnull().sum() >= 1]\nfor feature in features_with_nan:\n    print(feature,np.round(train[feature].isnull().mean()*100,5), '% missing values')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## statistics on numerical columns (Null cols)\ntrain[features_with_nan].select_dtypes(exclude='object').describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['Podcast_Name'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['Episode_Title'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['Genre'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['Publication_Day'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['Publication_Time'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['Episode_Sentiment'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(train.isnull().mean())*100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Imputing Null Values\n## for numerical values\n# Episode Length minutes\ntrain['Episode_Length_minutes'] = train['Episode_Length_minutes'].fillna(train['Episode_Length_minutes'].median())\n# Guest Popularity percentage\ntrain['Guest_Popularity_percentage'] = train['Guest_Popularity_percentage'].fillna(train['Guest_Popularity_percentage'].median())\n# Number of ads\ntrain['Number_of_Ads'] =  train['Number_of_Ads'].fillna(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(train.isnull().mean())*100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## For Test Data","metadata":{}},{"cell_type":"code","source":"## getting all different type of feature\nnum_features_test = [feature for feature in test.columns if  test[feature].dtype != 'O']\nprint(\"Number of numerical features : \",len(num_features_test))\ncat_features_test = [feature for feature in test.columns if test[feature].dtype == 'O']\nprint(\"Number of categorical features : \",len(cat_features_test))\ndiscrete_features_test = [feature for feature in num_features_test if len(test[feature].unique()) <= 25]\nprint(\"Number of discrete features : \",len(discrete_features_test))\ncontinuous_features_test  = [feature for feature in num_features_test if len(test[feature].unique()) > 25]\nprint(\"Number of continuous features :  \",len(continuous_features_test))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(test.isnull().mean())*100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Imputing Null Values\n## for numerical values\n# Episode Length minutes\ntest['Episode_Length_minutes'] = test['Episode_Length_minutes'].fillna(test['Episode_Length_minutes'].median())\n# Guest Popularity percentage\ntest['Guest_Popularity_percentage'] = test['Guest_Popularity_percentage'].fillna(test['Guest_Popularity_percentage'].median())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(test.isnull().mean())*100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Label Encoding","metadata":{}},{"cell_type":"code","source":"## Independent features and  dependent features\nx = train.drop(['Listening_Time_minutes'],axis=1)\ny = train['Listening_Time_minutes']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"le1 = LabelEncoder()\nle2 = LabelEncoder()\nle3  = LabelEncoder()\nle4 = LabelEncoder()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x['Podcast_Name'] = le1.fit_transform(x['Podcast_Name'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x['Episode_Title'] = le2.fit_transform(x['Episode_Title'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x['Genre'] = le3.fit_transform(x['Genre'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x['Publication_Day'] = le4.fit_transform(x['Publication_Day'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Encoding  and Scaling\n### One Hot Encoding for Columns which had lesser unique values and not ordinal ","metadata":{}},{"cell_type":"code","source":"## Creating Column Transformer with 3 types of transformer\nnum_features = ['Episode_Length_minutes','Host_Popularity_percentage','Guest_Popularity_percentage',\n 'Number_of_Ads']\nonehot_columns = ['Publication_Time','Episode_Sentiment']\n\nnumeric_transformer = StandardScaler()\noh_transformer = OneHotEncoder(drop='first')\n\npreprocessor = ColumnTransformer(\n    [\n        (\"OneHotEncoder\", oh_transformer, onehot_columns),\n        (\"StandardScaler\", numeric_transformer, num_features),\n        \n        \n    ],remainder='passthrough'\n    \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x  = preprocessor.fit_transform(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.DataFrame(x).head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## seperate dataset into train and test dataset \nx_train,x_cv,y_train,y_cv = train_test_split(x,y,test_size=0.25,random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_cv.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train_df = pd.DataFrame(x_train)\ny_train_df = pd.Series(y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the sample size (e.g., 80% of the original dataset)\nsample_size = int(0.8 * len(x_train_df))  # 10% of the original dataset\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Randomly sample the data\nx_train_sampled = x_train_df.sample(n=sample_size, random_state=42)\ny_train_sampled = y_train_df.sample(n=sample_size, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Create a function for Evaluation \ndef evaluate_model(true,pred):\n    mae = mean_absolute_error(true,pred)\n    mse = mean_squared_error(true,pred)\n    rmse = np.sqrt(mse)\n    score = r2_score(true,pred)\n    return mae , mse ,rmse ,score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Beginning  Model training\nmodels = {\n    \"Random Forest regresssor\": RandomForestRegressor(),\n    \"AdaBoost Regressor\" : AdaBoostRegressor(),\n    \"GradientBoost Regressor\": GradientBoostingRegressor(),\n    \"XGBoost Regressor\": XGBRegressor(),\n    \"LightGBM Regression\": LGBMRegressor(force_col_wise=True),\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_scores = {}\ntest_scores = {}\n\nfor i in range(len(list(models))):\n    model = list(models.values())[i]\n    model.fit(x_train_sampled,y_train_sampled) # Train model first dataset\n    \n\n    # Make Prediction\n    y_train_pred = model.predict(x_train_sampled)\n    y_cv_pred = model.predict(x_cv)   # Evaluate Train and Test dataset \n    model_train_mae , model_train_mse ,model_train_rmse ,model_train_r2 = evaluate_model(y_train_sampled, y_train_pred)\n\n    model_cv_mae , model_cv_mse ,model_cv_rmse ,model_cv_r2 =  evaluate_model(y_cv,y_cv_pred)\n\n    print(list(models.keys())[i])\n\n    print('Model performance for Training set')\n    print(\"- Mean Squared Error: {:.4f}\".format(model_train_mse))\n    print(\"- Root Mean Squared Error: {:.4f}\".format(model_train_rmse))\n    print(\"- Mean Absolute Error: {:.4f}\".format(model_train_mae))\n    print(\"- R2 Score: {:.4f}\".format(model_train_r2))\n    train_scores[list(models.keys())[i]] =  model_train_r2\n\n    print('-'*35)\n\n    print('Model performance for Test set')\n    print(\"- Mean Squared Error: {:.4f}\".format(model_cv_mse))\n    print(\"- Root Mean Squared Error: {:.4f}\".format(model_cv_rmse))\n    print(\"- Mean Absolute Error: {:.4f}\".format(model_cv_mae))\n    print(\"- R2 Score: {:.4f}\".format(model_cv_r2))\n    test_scores[list(models.keys())[i]] = model_cv_r2\n    \n    print('='*35)\n    print('\\n')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Initialize few parameter for Hyperparamter tuning\n\nrf_params = {\"max_depth\": [ele for ele in range(6,10,2)],\n             \"max_features\": [5, 7, 8],\n             \"min_samples_split\": [ele for ele in range(2,8,2)],\n             'criterion':['squared_error'],\n             \"n_estimators\": [ele for ele in range(200,500,50)]}\n\nxgboost_params = {\"learning_rate\": [0.1, 0.01],\n                  \"max_depth\": [ele for ele in range(6,10,2)],\n                  \"n_estimators\": [ele for ele in range(200,500,50)],\n                  \"colsample_bytree\": [round(i, 1) for i in np.arange(0.1, 0.6, 0.1)]}\n\nada_params = {\n    \"n_estimators\": [50,60,70,80],\n    'loss':['linear', 'square', 'exponential']\n}\n\ngradient_params={\"loss\": ['squared_error','huber','absolute_error'],\n             \"criterion\": ['friedman_mse','squared_error'],\n             \"min_samples_split\": [2, 8, 15, 12],\n             \"n_estimators\": [100, 200, 500],\n              \"max_depth\": [5, 8,  None, 10],\n            }\nlight_gbm_params = {\n    \"n_estimators\": [ele for ele in range(200,500,50)],\n    \"colsample_bytree\":[round(i, 1) for i in np.arange(0.1, 0.6, 0.1)],\n    \n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf_params","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Models list for HyperParameter Tuning \nrandomcv_models = [\n    (\"RF\", RandomForestRegressor(),rf_params),\n    (\"XG\",XGBRegressor(),xgboost_params),\n    # (\"ADA\",AdaBoostRegressor(),ada_params),\n    # (\"GRA\",GradientBoostingRegressor(),gradient_params),\n    (\"LIGHT\",LGBMRegressor(force_col_wise=True),light_gbm_params)\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"randomcv_models","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_param = {}\nfor name, model, params in randomcv_models:\n    random = RandomizedSearchCV(estimator=model,\n                                   param_distributions=params,\n                                   n_iter=10,\n                                   cv=3,\n                                   verbose=2,\n                                   n_jobs=-1)\n    random.fit(x_train, y_train)\n    model_param[name] = random.best_params_\n\nfor model_name in model_param:\n    print(f\"---------------- Best Params for {model_name} -------------------\")\n    print(model_param[model_name])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_param ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Retraining the models with best parameters\n### creating variables for parameter of the models \n\n#1. for RANDOM FOREST\nestimator = model_param['RF']['n_estimators']\nmin_sample_split = model_param['RF']['min_samples_split']\nmax_feature = model_param['RF']['max_features']\nmax_depths = model_param['RF']['max_depth']\n#2. for XGBOOST\nestimate = model_param['XGB']['n_estimators']\nlearning_rate_xgb = model_param['XGB']['learning_rate']\nmax_deep = model_param['XGB']['max_depth']\ncolsample_bytree_xgb = model_param['XGB']['colsample_bytree']\n#3 for lightgbm\nestimate_light = model_param['lightGBM']['n_estimators']\nlearning_rate_reg = model_param['lightGBM']['learning_rate ']\nmax_depth_light = model_param['lightGBM']['max_depth']\ncolsample_light = model_param['lightGBM']['colsample_bytree']\n\n\n## creating training score and testing score dictionaries\ntrain_best_score ,test_best_score = {} , {}\nmodels = {\n    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=estimator,min_samples_split=min_sample_split,\n                                                     max_features=max_feature,max_depth=max_depths, \n                                                     n_jobs=-1),\n     \"XGBBoost Regressor\": XGBRegressor(n_estimators=estimate,learning_rate=learning_rate_xgb,\n                                                     colsample_bytree = colsample_bytree_xgb,\n                                       max_depth=max_deep,n_jobs=-1),\n    \"LightGBM Regressor\":LGBMRegressor(n_estimators=estimate_light,learning_rate=learning_rate_reg,max_depth=max_depth_light,\n                                       colsample_bytree=colsample_light,n_jobs = -1)\n                                                          \n                                                          \n    \n}\naccuracy_score_train,accuracy_score_test, = {},{}\nfor i in range(len(list(models))):\n    model = list(models.values())[i]\n    model.fit(x_train, y_train) # Train model\n\n    # Make predictions\n    y_train_pred = model.predict(x_train)\n    y_test_pred = model.predict(x_test)\n\n   # Evaluate Train and Test dataset \n    model_train_mae , model_train_mse ,model_train_rmse ,model_train_r2 = evaluate_model(y_train, y_train_pred)\n\n    model_test_mae , model_test_mse ,model_test_rmse ,model_test_r2 =  evaluate_model(y_test,y_test_pred)\n\n    print(list(models.keys())[i])\n    \n    print('Model performance for Training set')\n    print(\"- Mean Squared Error: {:.4f}\".format(model_train_mse))\n    print(\"- Root Mean Squared Error: {:.4f}\".format(model_train_rmse))\n    print(\"- Mean Absolute Error: {:.4f}\".format(model_train_mae))\n    print(\"- R2 Score: {:.4f}\".format(model_train_r2))\n    train_best_score[model] = model_train_rmse\n    accuracy_score_train[model] = model_train_r2\n    \n\n    print('-'*35)\n\n    print('Model performance for Test set')\n    print(\"- Mean Squared Error: {:.4f}\".format(model_test_mse))\n    print(\"- Root Mean Squared Error: {:.4f}\".format(model_test_rmse))\n    print(\"- Mean Absolute Error: {:.4f}\".format(model_test_mae))\n    print(\"- R2 Score: {:.4f}\".format(model_test_r2))\n    test_best_score[model] = model_test_rmse\n    accuracy_score_test[model] = model_test_r2\n    \n    print('='*35)\n    print('\\n')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}